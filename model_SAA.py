import torch
import torch.nn as nn


class Self_Attn(nn.Module):
    """ Self attention Layer"""
    def __init__(self, in_dim):
        super().__init__()

        # Construct the module
        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//2 , kernel_size= 1)
        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//2 , kernel_size= 1)
        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)

        self.gamma = nn.Parameter(torch.zeros(1))
        self.softmax  = nn.Softmax(dim=-1)

    def forward(self,x):
        """
            inputs :
                x : input feature maps( B * C * W * H)
            returns :
                out : self attention value + input feature
                attention: B * N * N (N is Width*Height)
        """
        m_batchsize, C, width, height = x.size()

        proj_query  = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0,2,1) # B * N * C
        proj_key =  self.key_conv(x).view(m_batchsize, -1, width*height) # B * C * N
        energy =  torch.bmm(proj_query, proj_key) # batch matrix-matrix product

        attention = self.softmax(energy) # B * N * N
        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height) # B * C * N
        out = torch.bmm(proj_value, attention.permute(0,2,1)) # batch matrix-matrix product
        out = out.view(m_batchsize,C,width,height) # B * C * W * H

        out = self.gamma*out + x
        return out, attention

class Decoder(nn.Module):
    """
    Generator
    input:
        z: latent matrix with shape of (batch_size, 100)
    output:
        out: generated image with shape (batch_size, 1, 64, 64)
        p1: attention matrix generated by attn layer
    """
    def __init__(self, batch_size=64, attn=True, image_size=64, z_dim=100, conv_dim=64):
        super().__init__()
        self.attn = attn

        self.unflatten = nn.Unflatten(1, (128, 1, 1))

        layer1 = []
        layer1.append(nn.ConvTranspose2d(in_channels = 128, out_channels = 32, kernel_size = 4))
        layer1.append(nn.BatchNorm2d(32))
        layer1.append(nn.ReLU())
        self.l1 = nn.Sequential(*layer1)

        self.attn1 = Self_Attn(32)

        layer2 = []
        layer2.append(nn.ConvTranspose2d(in_channels = 32, out_channels = 8, kernel_size = 4, stride = 2, padding = 1))
        layer2.append(nn.BatchNorm2d(8))
        layer2.append(nn.ReLU())
        self.l2 = nn.Sequential(*layer2)

        self.attn2 = Self_Attn(8)

        layer3 = []
        layer3.append(nn.ConvTranspose2d(in_channels = 8, out_channels = 4, kernel_size = 8, stride = 4, padding = 2))
        layer3.append(nn.BatchNorm2d(4))
        layer3.append(nn.ReLU())
        self.l3 = nn.Sequential(*layer3)

        self.attn3 = Self_Attn(4)

        layer4 = []
        layer4.append(nn.ConvTranspose2d(in_channels = 4, out_channels = 4, kernel_size = 4, stride = 2, padding = 1))
        layer4.append(nn.BatchNorm2d(4))
        layer4.append(nn.Sigmoid())
        self.l4 = nn.Sequential(*layer4)


    def forward(self, z):
        out = self.unflatten(z)
        out=self.l1(out)
        if self.attn == True:
            out,_ = self.attn1(out)
        out=self.l2(out)
        if self.attn == True:
            out,_ = self.attn2(out)
        out=self.l3(out)
        if self.attn == True:
            out,_ = self.attn3(out)
        out=self.l4(out)

        return out.squeeze()


class Encoder(nn.Module):
    """
    Discriminator
    input:
        x: one batch of data with shape of (batch_size, 1, 64, 64)
    output:
        out.squeeze: a batch of scalars indicating the predict results
        p1: attention matrix generated by attn layer
    """
    def __init__(self, batch_size=64, attn=True, image_size=64, conv_dim=64):
        super().__init__()
        self.attn = attn

        layer1 = []
        layer1.append(nn.Conv2d(4, 1, 3, 1, 0))
        layer1.append(nn.LeakyReLU(0.1))
        self.l1 = nn.Sequential(*layer1)

        self.attn1 = Self_Attn(1)

        layer2 = []
        layer2.append(nn.Linear(3844, 1024))
        layer2.append(nn.LeakyReLU(0.1))
        self.l2 = nn.Sequential(*layer2)

        self.attn2 = Self_Attn(1)

        layer3 = []
        layer3.append(nn.Linear(1024, 128))
        layer3.append(nn.LeakyReLU(0.1))
        self.l3 = nn.Sequential(*layer3)

        self.flatten = nn.Flatten()

        layer4 = []
        layer4.append(nn.Linear(36, 32))
        layer4.append(nn.LeakyReLU(0.1))
        self.l4 = nn.Sequential(*layer4)


    def forward(self, x):
        out = self.l1(x)
        out = self.flatten(out)
        if self.attn == True:
            out,_ = self.attn1(out)
        out = self.l2(out)
        if self.attn == True:
            out,_ = self.attn2(out)
        out = self.l3(out)
        out = self.l4(out)

        return out.squeeze()

class Autoencoder(nn.Module):
    def __init__(self, batch_size=64, attn=True, image_size=64):
        super().__init__()
        self.attn = attn
        self.encoder = Encoder(batch_size=batch_size, attn=attn, image_size=image_size)
        self.decoder = Decoder(batch_size=batch_size, attn=attn, image_size=image_size)

    def forward(self, x):
        z = self.encoder(x)
        out = self.decoder(z)
        return out, z