{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noKAAn8vDXN-"
   },
   "source": [
    "# **Создание датасета**\n",
    "P.s. Только при первом запуске"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "dXohIZc9lnan"
   },
   "outputs": [],
   "source": [
    "run_gnuplot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iyoq8cLTzLJ_",
    "outputId": "4dfe63e0-4d25-4de5-f96c-06e683a654a4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifaUUT4S0XzB",
    "outputId": "150e9943-f97b-4f1c-e7b7-13625c2b9736"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"ls\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/MyDrive/Chemistry-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9gDJPOwIvYi"
   },
   "outputs": [],
   "source": [
    "#!ls /usr/bin/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYx4GRv_MFMF"
   },
   "outputs": [],
   "source": [
    "if run_gnuplot:\n",
    "    %cd /content/drive/MyDrive/Chemistry-main/Data/\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rM4SzXfQGrFh"
   },
   "outputs": [],
   "source": [
    "if run_gnuplot:\n",
    "    !bash /content/drive/MyDrive/Chemistry-main/Data/really_all_region.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "-ssrTfCykva8"
   },
   "outputs": [],
   "source": [
    "if run_gnuplot:\n",
    "    import os\n",
    "    for file_name in os.listdir('/content/drive/MyDrive/Chemistry-main'):\n",
    "        # some filter\n",
    "        if 'result' not in file_name or 'contour' not in file_name:\n",
    "            continue\n",
    "        print('rename', file_name)\n",
    "        new_file_name = file_name.replace('.csv', '_')\n",
    "        os.rename(file_name, new_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW-zkuE5DseO"
   },
   "source": [
    "# **Инициализация**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrnVcZbVhFwq",
    "outputId": "af19120e-8623-4c45-a768-94f893ed2417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] Системе не удается найти указанный путь: '/content/drive/MyDrive/Chemistry-main/'\n",
      "C:\\Users\\Egor\\Desktop\\study\\project\\Chemistry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Egor\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "\"pwd\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/Chemistry-main/\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NKtQaqY2hnl3"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from utils import seed, get_train_dataloader, get_test_dataloader, model_to_log, split_x_train, load_data_phase, read_phase_file\n",
    "from model import get_model, load_model\n",
    "from norm_denorm import calculate_mean_disp, norm, denorm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vOD_Y29nh1FB"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-4\n",
    "epochs = 200\n",
    "batch_size = 31\n",
    "step_size = 20\n",
    "gamma = 0.99\n",
    "# parameters to be predicted\n",
    "params = ['V', 'D', 'tp', 'tb']  # 'V', 'D', 'tp', 'tb'\n",
    "\n",
    "# path to folder and training set\n",
    "train_folder = '/content/drive/MyDrive/Chemistry-main/Data'\n",
    "test_folder = '/content/drive/MyDrive/Chemistry-main/Test'\n",
    "# file name to save mean and variance for normalization\n",
    "mean_disp_file = 'mean_disp.json'\n",
    "\n",
    "# phase pattern size\n",
    "phase_size = (64, 64)\n",
    "\n",
    "config = {\n",
    "    \"lr\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"bs\": batch_size,\n",
    "    \"sched\": 'StepLR',\n",
    "    \"sched_ss\": step_size,\n",
    "    \"sched_g\": gamma,\n",
    "    \"params\": '_'.join(params),\n",
    "    \"optim\": 'Adam',\n",
    "    \"size\": phase_size[0],\n",
    "}\n",
    "important_keys = ['optim', 'bs', 'params']\n",
    "config['exp_name'] = 'InceptionResnetV1_' + '_'.join(map(lambda key: f'{key}={config[key]}', important_keys))\n",
    "model_file_name = config['exp_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vRX4XuGiLiC",
    "outputId": "0066c21a-1da8-44f8-e9b3-e0257a29b28c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36089379 0.18357112 0.52197279 0.80557017] [5.14534221e-05 4.60363108e-03 4.85982284e-03 1.19889746e-03]\n"
     ]
    }
   ],
   "source": [
    "from norm_denorm import dump_mean_disp\n",
    "\n",
    "mean, dispersion = calculate_mean_disp(train_folder, params)\n",
    "print(mean, dispersion)\n",
    "\n",
    "# maintaining the average and variance of usage with an already trained network\n",
    "# dump_mean_disp(mean_disp_file, mean, dispersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rShuIwPBqMCb"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = load_data_phase(train_folder, params, phase_size)\n",
    "y_train = norm(y_train, mean, dispersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-yCT-pYX94RH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val files [ 20  32  53  67  77 114 119 131 142 146 160 161 170 175 186 188 189 190\n",
      " 193 223 226 227 243 278 305 329 378 387 400 402 417]\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val = map(lambda x: torch.tensor(x, dtype=torch.float, device=device), split_x_train(x_train, y_train, 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LbfmbqEvDFgO"
   },
   "outputs": [],
   "source": [
    "val_dataloader = get_test_dataloader(x_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ILyV6A1mrID5"
   },
   "outputs": [],
   "source": [
    "train_dataloader = get_train_dataloader(x_train, y_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7eG5UIBaEyb9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eJXjZ5wPEHCE"
   },
   "outputs": [],
   "source": [
    "# импорт моделей, оставить что-то одно\n",
    "#from model_InceptionResnetV1 import Autoencoder # параметр device, по умалчанию None\n",
    "#from model_SAA import Autoencoder # параметр attn=True/False, включает или выключает слои самовнимания, по умалчанию True\n",
    "from model_resnet import Autoencoder # параметр net, по умалчанию net=\"resnet50\". Возможные варианты: 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnAGHS9CEp2S"
   },
   "source": [
    "# **Обучение**\n",
    "\n",
    "Переменная load_prev_model говорит о том, нужно ли продолжать обучение со старыми весами, или начать с начала.\n",
    "\n",
    "В training_model выбираем что обучаем, автоэнкодер(AE) или модель предсказывающаю параметры по автоэнкодеру(param_AE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "goNTjBs2GhNG"
   },
   "outputs": [],
   "source": [
    "load_prev_model = False\n",
    "training_model = \"AE\" # [\"AE\", \"param_AE\", \"param\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zZisNVys45rH"
   },
   "outputs": [],
   "source": [
    "# Для обучение предсказания параметров по обученному автоэнкодеру\n",
    "def train_epoch_param(model, encoder, loss, optimizer, dataloader, do_train):\n",
    "    running_loss = 0.\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward and backward\n",
    "        with torch.set_grad_enabled(do_train):\n",
    "            # 4\n",
    "            # preds, _, _ = encoder(inputs)\n",
    "            # 3\n",
    "            # preds = encoder(inputs)\n",
    "            #\n",
    "            _, z = encoder(inputs)\n",
    "            # print(preds.size())\n",
    "            preds = model(z)\n",
    "            loss_value = loss(preds, targets)\n",
    "\n",
    "            # backward + optimize only if in training phase\n",
    "            if do_train:\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss_value.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def train_model_param(model, encoder, loss, optimizer, scheduler, train_dataloader, val_dataloader, num_epochs, config, start_epoch=-1, prev_loss=None):\n",
    "    if prev_loss:\n",
    "      best_val_loss = prev_loss\n",
    "    else:\n",
    "      best_val_loss = math.inf\n",
    "    save__original = True\n",
    "    for epoch in range(start_epoch+1, num_epochs):\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                dataloader = train_dataloader\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                dataloader = val_dataloader\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            epoch_loss = train_epoch_param(model, encoder, loss, optimizer, dataloader, phase == 'train')\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            print('loss', epoch_loss, 'epoch', epoch)\n",
    "\n",
    "            if phase == 'val' and best_val_loss > epoch_loss:\n",
    "                print('New best val loss', epoch_loss, 'epoch', epoch)\n",
    "                best_val_loss = epoch_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': epoch_loss,\n",
    "                    'model_name': config['exp_name']\n",
    "                }, f'./models/param_{config[\"exp_name\"]}.pth')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zH2of41VFRuu"
   },
   "outputs": [],
   "source": [
    "# Для обучение Автоэнкодера\n",
    "def train_epoch(model, loss, optimizer, dataloader, do_train):\n",
    "    running_loss = 0.\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    # Iterate over data.\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward and backward\n",
    "        with torch.set_grad_enabled(do_train):\n",
    "            preds, _ = model(inputs)\n",
    "            loss_value = loss(preds, inputs)\n",
    "\n",
    "            # backward + optimize only if in training phase\n",
    "            if do_train:\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss_value.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def train_model(model, loss, optimizer, scheduler, train_dataloader, val_dataloader, num_epochs, config, start_epoch=-1, prev_loss=None):\n",
    "    if prev_loss:\n",
    "      best_val_loss = prev_loss\n",
    "    else:\n",
    "      best_val_loss = math.inf\n",
    "    save__original = True\n",
    "    for epoch in range(start_epoch+1, num_epochs):\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                dataloader = train_dataloader\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                dataloader = val_dataloader\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            epoch_loss = train_epoch(model, loss, optimizer, dataloader, phase == 'train')\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            print('loss', epoch_loss, 'epoch', epoch)\n",
    "\n",
    "            if phase == 'val' and best_val_loss > epoch_loss:\n",
    "                print('New best val loss', epoch_loss, 'epoch', epoch)\n",
    "                best_val_loss = epoch_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': epoch_loss,\n",
    "                    'model_name': config['exp_name']\n",
    "                }, f'./models/{config[\"exp_name\"]}.pth')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aIRiYAP1o9Bj"
   },
   "outputs": [],
   "source": [
    "if training_model == \"param_AE\":\n",
    "    # загрузка весов автоэнкодера\n",
    "    autoencoder_state_dict = torch.load(f'./models/{config[\"exp_name\"]}.pth')['model_state_dict']\n",
    "    AE = Autoencoder()\n",
    "    AE.load_state_dict(autoencoder_state_dict)\n",
    "    AE.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        AE.cuda()\n",
    "\n",
    "    del autoencoder_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "T84CYCKABUH3"
   },
   "outputs": [],
   "source": [
    "if training_model == \"param_AE\":\n",
    "    model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1792, 512),\n",
    "            # nn.Dropout(p=0.01),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, len(params))\n",
    "        )\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    # loss_func = torch.nn.L1Loss()\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gRM87X8yKMXS"
   },
   "outputs": [],
   "source": [
    "if training_model == \"param_AE\":\n",
    "    save = torch.load(f'./models/{config[\"exp_name\"]}.pth')\n",
    "    if load_prev_model:\n",
    "      start_epoch = save['epoch']\n",
    "      start_loss = save['loss']\n",
    "    else:\n",
    "      start_epoch = -1\n",
    "      start_loss = None\n",
    "\n",
    "    print(start_epoch, start_loss)\n",
    "\n",
    "    if load_prev_model:\n",
    "        model.load_state_dict(save['model_state_dict'])\n",
    "        optimizer.load_state_dict(save['optimizer_state_dict'])\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-ucaoQaC86PY"
   },
   "outputs": [],
   "source": [
    "if training_model == \"param_AE\":\n",
    "    model = train_model_param(model, AE, loss_func, optimizer, scheduler, train_dataloader, val_dataloader, epochs, config, start_epoch, start_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "lhodRJ9VKxtl"
   },
   "outputs": [],
   "source": [
    "if training_model == \"AE\":\n",
    "    # не забываем указать параметры, в зависимости от архитектуры\n",
    "    model = Autoencoder()\n",
    "\n",
    "    # loss_func = torch.nn.L1Loss()\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0GreEZ0aLE9O"
   },
   "outputs": [],
   "source": [
    "#if training_model == \"AE\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "AvzZapqvrdhy"
   },
   "outputs": [],
   "source": [
    "if training_model == \"AE\":\n",
    "    if load_prev_model:\n",
    "      save = torch.load(f'./models/{config[\"exp_name\"]}.pth')\n",
    "      model.load_state_dict(save['model_state_dict'])\n",
    "      optimizer.load_state_dict(save['optimizer_state_dict'])\n",
    "      model.train()\n",
    "      start_epoch = save['epoch']\n",
    "      prev_loss = save['loss']\n",
    "      print('loss ', prev_loss)\n",
    "      print('start_epoch ', start_epoch)\n",
    "      del save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "7MX-1NVOrlNB"
   },
   "outputs": [],
   "source": [
    "if training_model == \"AE\":\n",
    "    if load_prev_model:\n",
    "      model = train_model(model, loss_func, optimizer, scheduler, train_dataloader, val_dataloader, epochs, config, start_epoch, prev_loss)\n",
    "    else:\n",
    "      model = train_model(model, loss_func, optimizer, scheduler,train_dataloader, val_dataloader, epochs, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfCAy2rvGQKS"
   },
   "source": [
    "# **Тестирование и метрики**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zCKg2Nr0CFg8"
   },
   "outputs": [],
   "source": [
    "x_test, y_test = load_data_phase(test_folder, params, phase_size)\n",
    "y_test = norm(y_test, mean, dispersion)\n",
    "x_test, y_test = map(lambda x: torch.tensor(x, dtype=torch.float, device=device), (x_test, y_test))\n",
    "test_dataloader = get_train_dataloader(x_test, y_test, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bShY_CbuGXcn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Egor\\AppData\\Local\\Temp\\ipykernel_18524\\1839393347.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  autoencoder_state_dict = torch.load(f'./models/{config[\"exp_name\"]}.pth')['model_state_dict']\n"
     ]
    }
   ],
   "source": [
    "autoencoder_state_dict = torch.load(f'./models/{config[\"exp_name\"]}.pth')['model_state_dict']\n",
    "AE = Autoencoder()\n",
    "AE.load_state_dict(autoencoder_state_dict)\n",
    "AE.eval()\n",
    "if torch.cuda.is_available():\n",
    "    AE.cuda()\n",
    "\n",
    "del autoencoder_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "Q-XSV_h2CkJk"
   },
   "outputs": [],
   "source": [
    "best_model = get_model(x_test,y_test)\n",
    "best_model.load_state_dict(torch.load(f'./models/{config[\"exp_name\"]}.pth', weights_only=True)['model_state_dict'], strict=False)\n",
    "best_model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    best_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "5pk4KPuyUAM7"
   },
   "outputs": [],
   "source": [
    "# сохранение оригинальной и созданой AE фазовой диаграммы\n",
    "for inputs, targets in test_dataloader:\n",
    "  save_image(inputs, os.path.join('./test_img', f'original_AE_IRN1.png'))\n",
    "  save_image(AE(inputs)[0], os.path.join('./test_img', 'fake_AE_IRN1.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "6vDcGHGuKkXX"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (388x8192 and 1792x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_train_result \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m      2\u001b[0m y_val_result \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mforward(AE(x_val)[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m      3\u001b[0m y_test_result \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mforward(AE(x_test)[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (388x8192 and 1792x512)"
     ]
    }
   ],
   "source": [
    "y_train_result = best_model.forward(AE(x_train)[1]).detach()\n",
    "y_val_result = best_model.forward(AE(x_val)[1]).detach()\n",
    "y_test_result = best_model.forward(AE(x_test)[1]).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "GvbUOny5Gx9Q"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m actual \u001b[38;5;241m=\u001b[39m inputs[i][j][k1][k2]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m actual \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 11\u001b[0m     TP \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m predicted \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m actual \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     13\u001b[0m     TN \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FP, FN, TP, TN = 0, 0, 0, 0\n",
    "for inputs, targets in test_dataloader:\n",
    "    fake, z = AE(inputs)\n",
    "    for i in range(len(inputs)):\n",
    "        for j in range(len(inputs[i])):\n",
    "            for k1 in range(len(inputs[i][j])):\n",
    "                for k2 in range(len(inputs[i][j][k1])):\n",
    "                    predicted = round(float(fake[i][j][k1][k2]))\n",
    "                    actual = inputs[i][j][k1][k2]\n",
    "                    if predicted == 1 and actual == 1:\n",
    "                        TP += 1\n",
    "                    elif predicted == 0 and actual == 0:\n",
    "                        TN += 1\n",
    "                    elif actual == 1 and predicted == 0:\n",
    "                        FN += 1\n",
    "                    elif actual == 0 and predicted == 1:\n",
    "                        FP += 1\n",
    "    # print(FP, FN, TP, TN)\n",
    "print(\"f1 =\", 2*TP / (2*TP + FP + FN))\n",
    "print(\"IoU =\", TP / (TP + FP + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4euePrvribn"
   },
   "outputs": [],
   "source": [
    "y_train_result_np = np.array(y_train_result)\n",
    "y_val_result_np = y_val_result.cpu().numpy()\n",
    "y_test_result_np = y_test_result.detach().cpu().numpy()\n",
    "\n",
    "y_train_np = y_train.detach().cpu().numpy()\n",
    "y_val_np = y_val.detach().cpu().numpy()\n",
    "y_test_np = y_test.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKh6sVAmrlVG"
   },
   "outputs": [],
   "source": [
    "def add_subplot(fig, train, result, param_index, plot_index, label):\n",
    "\n",
    "    sort = train[:, param_index].argsort(axis=0).reshape((-1,))\n",
    "    ax_train = fig.add_subplot(3, 1, plot_index)\n",
    "    ax_train.set_title(label=label, fontsize=12)\n",
    "    ax_train.set_ylabel(param)\n",
    "    ax_train.set_xlabel('num File')\n",
    "    ax_train.plot(denorm(train[sort], mean, dispersion)[:, param_index], 'go', label='train')\n",
    "    ax_train.plot(denorm(result[sort], mean, dispersion)[:, param_index], 'ro', label='result')\n",
    "    ax_train.legend()\n",
    "\n",
    "def show(param_index, param):\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    fig.suptitle(f'Параметр {param}', fontsize=16)\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    add_subplot(fig, y_train_np, y_train_result_np, param_index, 1, 'Обучающий набор')\n",
    "    add_subplot(fig, y_val_np, y_val_result_np, param_index, 1, 'Валидационный набор')\n",
    "    add_subplot(fig, y_test_np, y_test_result_np, param_index, 2, 'Тестовый набор')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvqVh7kOrmEv"
   },
   "outputs": [],
   "source": [
    "for param_index, param in enumerate(params):\n",
    "    show(param_index, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZM7FMniTXLe"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def get_loss(result, real, i):\n",
    "    error_m = torch.nn.MSELoss()(real[:, i], result[:, i]).item()\n",
    "    error_a = torch.nn.L1Loss()(real[:, i], result[:, i]).item()\n",
    "    r2 = r2_score(real[:, i], result[:, i])\n",
    "    return round(error_m, 6), round(error_a, 6), round(r2, 6)\n",
    "\n",
    "def print_errors(i):\n",
    "    train_error_m, train_error_a, r2 = get_loss(y_train_result, y_train, i)\n",
    "    print('train error L1', train_error_a, 'MSE', train_error_m, 'r2', r2)\n",
    "    val_error_m, val_error_a, r2 = get_loss(y_val_result, y_val, i)\n",
    "    print('val error L1', val_error_a, 'MSE', val_error_m, 'r2', r2)\n",
    "    test_error_m, test_error_a, r2 = get_loss(y_test_result, y_test, i)\n",
    "    print('test error L1', test_error_a, 'MSE', test_error_m, 'r2', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "povX8iQoTXLe"
   },
   "outputs": [],
   "source": [
    "for param_index, param in enumerate(params):\n",
    "    print(param, 'errors')\n",
    "    print_errors(param_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkG6q5lgRLXu"
   },
   "outputs": [],
   "source": [
    "r2_score(y_test, y_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtvHo09iC-pL"
   },
   "outputs": [],
   "source": [
    "r2_score(y_val, y_val_result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "noKAAn8vDXN-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
